{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c3f33463-dae5-4ad3-b189-f83f811dbd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conllu import parse\n",
    "from collections import defaultdict\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a4906d39-f65e-4c00-b755-18dbd20e92ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "files = []\n",
    "for d, s, f in os.walk('amalgum'):\n",
    "    for file in f:\n",
    "        if file.endswith(\"conllu\"):\n",
    "            files.append(f\"{d}/{file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "18294c5f-f730-47fa-a924-b2dab556e9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4288/4288 [02:10<00:00, 32.90it/s]\n"
     ]
    }
   ],
   "source": [
    "tags = defaultdict(list)\n",
    "words = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for file in tqdm.tqdm(files):\n",
    "    with open(file) as f:\n",
    "        sentences = parse(f.read())\n",
    "        for sentence in sentences:\n",
    "            for token in sentence:\n",
    "                if token['form'] not in tags[token['xpos']]:\n",
    "                    tags[token['xpos']].append(token['form'])\n",
    "                for token2 in sentence:\n",
    "                    if token['form'] != token2['form']:\n",
    "                        words[token['form']][token2['form']] += 1\n",
    "                        words[token2['form']][token['form']] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "818e417f-6756-4c86-bd8b-c368f6728169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(dict(tags), open(\"tags\", \"wb\"))\n",
    "pickle.dump(dict(words), open(\"words\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d31f2075-563d-4d1a-8f3d-aea97de5adb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llr import llr_2x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "28a4d8c1-a8b9-43c4-b45f-b8d60b81e5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 4288/4288 [00:31<00:00, 135.29it/s]\n"
     ]
    }
   ],
   "source": [
    "all_sents = []\n",
    "for file in tqdm.tqdm(files):\n",
    "    with open(file) as f:\n",
    "        sentences = parse(f.read())\n",
    "        for sentence in sentences:\n",
    "            all_sents.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f0e59c2-e37b-4bf8-91b2-901df9d54a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(all_sents, open(\"all_sents\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "643d66f8-18ef-4c6f-9f1c-013e29365b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "tags = pickle.load(open(\"tags\", \"rb\"))\n",
    "words = pickle.load(open(\"words\", \"rb\"))\n",
    "all_sents = pickle.load(open(\"all_sents\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bf29e04-66e8-4e9f-b38b-f71dc1f4812f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(all_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e51f8e00-5c1e-4ebd-a243-1a66647a4f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = \"\"\"AMALGUM_whow_spurs-36\n",
    "AMALGUM_academic_mitotane-25\n",
    "AMALGUM_academic_liveness-20\n",
    "AMALGUM_academic_flipped-13\n",
    "AMALGUM_news_independence-35\n",
    "AMALGUM_voyage_ville-92\n",
    "AMALGUM_whow_podcast-38\n",
    "AMALGUM_academic_aggression-4\n",
    "AMALGUM_fiction_laputa-32\n",
    "AMALGUM_fiction_daddy-54\n",
    "AMALGUM_voyage_bolivia-45\n",
    "AMALGUM_whow_gardenia-58\n",
    "AMALGUM_fiction_venice-19\n",
    "AMALGUM_academic_mediating-7\n",
    "AMALGUM_fiction_desplein-6\n",
    "AMALGUM_bio_gould-7\n",
    "AMALGUM_voyage_iringa-61\n",
    "AMALGUM_whow_hostas-26\n",
    "AMALGUM_fiction_submarine-10\n",
    "AMALGUM_interview_literary-28\n",
    "AMALGUM_voyage_kilkenny-99\n",
    "AMALGUM_whow_yews-11\n",
    "AMALGUM_academic_prevalence-4\n",
    "AMALGUM_fiction_reef-7\n",
    "AMALGUM_voyage_gizella-65\n",
    "AMALGUM_bio_tonti-20\n",
    "AMALGUM_interview_cynthia-29\n",
    "AMALGUM_academic_vector-29\n",
    "AMALGUM_news_morsi-19\n",
    "AMALGUM_academic_propositional-29\n",
    "AMALGUM_academic_membranes-13\n",
    "AMALGUM_bio_lewin-20\n",
    "AMALGUM_fiction_yates-70\n",
    "AMALGUM_whow_reflect-77\n",
    "AMALGUM_fiction_hawser-15\n",
    "AMALGUM_news_unemployment-32\n",
    "AMALGUM_academic_disclosure-6\n",
    "AMALGUM_news_sledge-11\n",
    "AMALGUM_voyage_delhi-88\n",
    "AMALGUM_bio_aero-22\n",
    "AMALGUM_fiction_dolly-70\n",
    "AMALGUM_bio_headwards-3\n",
    "AMALGUM_news_fulham-31\n",
    "AMALGUM_whow_hair-15\n",
    "AMALGUM_interview_suzanne-6\n",
    "AMALGUM_news_bangladesh-8\n",
    "AMALGUM_whow_references-19\n",
    "AMALGUM_fiction_eddie-26\n",
    "AMALGUM_bio_tupoumoheofo-46\n",
    "AMALGUM_interview_leanne-19\n",
    "AMALGUM_interview_cailliau-12\n",
    "AMALGUM_news_inaugural-15\n",
    "AMALGUM_voyage_timor-60\n",
    "AMALGUM_academic_sequestration-16\n",
    "AMALGUM_interview_rickard-6\n",
    "AMALGUM_bio_fenn-35\n",
    "AMALGUM_news_fined-19\n",
    "AMALGUM_interview_waterfront-13\n",
    "AMALGUM_whow_graphic-29\n",
    "AMALGUM_news_nationality-23\n",
    "AMALGUM_voyage_kununurra-61\n",
    "AMALGUM_voyage_hollywood-29\n",
    "AMALGUM_academic_cloudlet-9\n",
    "AMALGUM_bio_deventer-8\n",
    "AMALGUM_interview_ontario-4\n",
    "AMALGUM_voyage_chaung-76\n",
    "AMALGUM_interview_traralgon-21\n",
    "AMALGUM_bio_roncalli-21\n",
    "AMALGUM_news_freed-9\n",
    "AMALGUM_bio_fludd-18\n",
    "AMALGUM_whow_facilitator-8\n",
    "AMALGUM_interview_tax-43\n",
    "AMALGUM_voyage_logan-23\n",
    "AMALGUM_fiction_disputing-35\n",
    "AMALGUM_bio_vancouver-18\n",
    "AMALGUM_whow_einkorn-98\n",
    "AMALGUM_academic_usability-7\n",
    "AMALGUM_bio_blocke-41\n",
    "AMALGUM_whow_artist-72\n",
    "AMALGUM_voyage_glossop-68\n",
    "AMALGUM_academic_phflts-20\n",
    "AMALGUM_academic_integrating-25\n",
    "AMALGUM_voyage_morenci-13\n",
    "AMALGUM_interview_tommy-14\n",
    "AMALGUM_whow_beans-51\n",
    "AMALGUM_news_christmas-1\n",
    "AMALGUM_bio_aliqoli-19\n",
    "AMALGUM_academic_cdte-5\n",
    "AMALGUM_news_magnitude-31\n",
    "AMALGUM_news_usa-4\n",
    "AMALGUM_interview_reed-37\n",
    "AMALGUM_interview_theater-2\n",
    "AMALGUM_fiction_mansfield-28\n",
    "AMALGUM_academic_criteria-21\n",
    "AMALGUM_fiction_matilda-15\n",
    "AMALGUM_academic_connectivity-10\n",
    "AMALGUM_interview_stricker-12\n",
    "AMALGUM_whow_celebrate-11\n",
    "AMALGUM_bio_tisquesusa-23\"\"\".split(\"\\n\")\n",
    "\n",
    "for i in range(2):\n",
    "    sent = []\n",
    "    while len(sent) < 5 or len(sent) > 50:\n",
    "        sent = random.choice(all_sents)\n",
    "    sents.append(sent.metadata['sent_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "18e070bc-56d2-44b3-ad54-eb982e5c763a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 101/101 [00:06<00:00, 16.75it/s]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open(\"devset.csv\", \"w\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=['sent_id', 'Original Sentence', 'New Sentence'])\n",
    "    writer.writeheader()\n",
    "\n",
    "    for sent_id in tqdm.tqdm(sents):\n",
    "        sent = [i for i in all_sents if i.metadata['sent_id'] == sent_id]\n",
    "        if sent:\n",
    "            sent = sent[0]\n",
    "            writer.writerow({'sent_id': sent_id, 'Original Sentence': sent.metadata['text']})\n",
    "        else:\n",
    "            print(\"can't find\", sent_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fa36da3f-ea3f-447f-af8e-5d48bfba1316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency(word):\n",
    "    return sum(words[word].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b3ccba32-f2d9-413a-bec5-813ea2332449",
   "metadata": {},
   "outputs": [],
   "source": [
    "k22a = 0\n",
    "for k, v in words.items():\n",
    "    k22a += sum(v.values())\n",
    "k22a //= 2\n",
    "def get_llr(w1, w2):\n",
    "    k11 = words[w1][w2]\n",
    "    k12 = sum(words[w1].values()) - k11\n",
    "    k21 = sum(words[w2].values()) - k11\n",
    "    k22 = k22a - k12 - k21 - k11\n",
    "    return llr_2x2(k11, k12, k21, k22)    \n",
    "\n",
    "def get_sentence_llr(sent):\n",
    "    total = 0\n",
    "    for token1 in sent:\n",
    "        for token2 in sent:\n",
    "            total += get_llr(token1, token2)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "48f8b0dc-0cc1-4fe4-9ce7-667c21661468",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = { k: list(set(v)) for k, v in tags.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9dfb8609-1bd1-4c0e-a30d-af2b0b850f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(token):\n",
    "    return not(token['xpos'] in ['DT', 'IN', 'CC', '.', 'PRP', 'PDT', 'TO', 'PRP$'] or token['upos'] in ['AUX', 'PUNCT'])\n",
    "\n",
    "def get_replacements(sent_id):\n",
    "    sent = [i for i in all_sents if i.metadata['sent_id'] == sent_id][0]\n",
    "    # print(\"got sent\", sent)\n",
    "    repl = []\n",
    "    for token in sent:\n",
    "        if not replace(token):\n",
    "            continue\n",
    "        else:\n",
    "            w = None\n",
    "            while not w or frequency(w) < 10000 or len(w) < 3:\n",
    "                w = random.choice(tags[token['xpos']])\n",
    "            # print(\"replacing\", token['form'], token['xpos'], 'with', w)\n",
    "            repl.append(w)\n",
    "    return repl\n",
    "\n",
    "def do_replacements(sent_id, repls):\n",
    "    sent = [i for i in all_sents if i.metadata['sent_id'] == sent_id][0]\n",
    "    # print(\"got sent\", sent)\n",
    "    new_sent = []\n",
    "    i = 0\n",
    "    for token in sent:\n",
    "        if not replace(token):\n",
    "            new_sent.append(token['form'])\n",
    "        else:\n",
    "            new_sent.append(repls[i])\n",
    "            i += 1\n",
    "    return new_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0d79b8f4-921f-4158-91cd-e52cd3775451",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|█████████████████▍                        | 42/101 [02:24<03:22,  3.44s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[114], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m sent_id \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msent_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      8\u001b[0m orig \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal Sentence\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 10\u001b[0m replacement_candidates \u001b[38;5;241m=\u001b[39m [\u001b[43mget_replacements\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent_id\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m)]\n\u001b[1;32m     11\u001b[0m best_candidate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(replacement_candidates, key\u001b[38;5;241m=\u001b[39mget_sentence_llr)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# for i in replacement_candidates:\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# print(' '.join(do_replacements(sent_id, i)))\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# print(\"llr\", get_sentence_llr(i))\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[112], line 13\u001b[0m, in \u001b[0;36mget_replacements\u001b[0;34m(sent_id)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m     w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m w \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mfrequency\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m10000\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(w) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m     14\u001b[0m         w \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(tags[token[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxpos\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# print(\"replacing\", token['form'], token['xpos'], 'with', w)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[63], line 2\u001b[0m, in \u001b[0;36mfrequency\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrequency\u001b[39m(word):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "rows = []\n",
    "with open(\"devset.csv\") as devset:\n",
    "    reader = csv.DictReader(devset)\n",
    "    for row in tqdm.tqdm(reader, total=len(sents)):\n",
    "        sent_id = row[\"sent_id\"]\n",
    "        orig = row[\"Original Sentence\"]\n",
    "\n",
    "        replacement_candidates = [get_replacements(sent_id) for _ in range(10)]\n",
    "        best_candidate = max(replacement_candidates, key=get_sentence_llr)\n",
    "        # for i in replacement_candidates:\n",
    "            # print(' '.join(do_replacements(sent_id, i)))\n",
    "            # print(\"llr\", get_sentence_llr(i))\n",
    "        new_sentence = ' '.join(do_replacements(sent_id, best_candidate))\n",
    "        # input()\n",
    "        rows.append({\"sent_id\": sent_id, \"Original Sentence\": orig, \"New Sentence\": new_sentence})\n",
    "\n",
    "with open(\"devset.csv\", \"w\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=['sent_id', 'Original Sentence', 'New Sentence'])\n",
    "    writer.writeheader()\n",
    "\n",
    "    for row in tqdm.tqdm(rows):\n",
    "        writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d2c52c56-1465-4d3c-91af-0de304de6edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 42/42 [00:00<00:00, 59134.20it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(\"devset.csv\", \"w\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=['sent_id', 'Original Sentence', 'New Sentence'])\n",
    "    writer.writeheader()\n",
    "\n",
    "    for row in tqdm.tqdm(rows):\n",
    "        writer.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
